{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-714 Homework 2\n",
    "\n",
    "In this homework, you will be implementing a neural network library in the needle framework. Reminder: __you must save a copy in drive__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set up the assignment\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p 10714\n",
    "%cd /content/drive/MyDrive/10714\n",
    "!git clone https://github.com/dlsys10714/hw2.git\n",
    "%cd /content/drive/MyDrive/10714/hw2\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework builds off of Homework 1. First, in your Homework 2 directory, go to the files `autograd.py`, `ops.py` and `numpy_backend.py` in the `python/needle` directory, and fill in the code between `### BEGIN YOUR SOLUTION` and `### END YOUR SOLUTION` with your solutions from Homework 1. \n",
    "\n",
    "__A note__: When copying over your solutions from the previous `numpy_backend.py`, you should add something like `.astype(inputs[0].dtype)` to your methods' return statements to ensure t
    your methods don't change the dtype (in fact, you can directly use the previous snippet in all cases). This is because functions like `np.divide` will change their output type in order to present a more accurate answer. However, this may result in type conflicts in our current version of needle. Forcibly casting like this is not the optimal solution, and we will probably revisit it later, but it is an acceptable workaround for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 0 [5 points]\n",
    "\n",
    "Before you begin implementing your Needle neural network library, you first have to implement a couple more ops. These are crucial ops for the loss function and optimizers that you will quickly find yourself needing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PowerScalar\n",
    "\n",
    "We will generally find it crucial to find the elementwise power (to a scalar) of a tensor, for example, to compute the variance in BatchNorm. You will also need to implement the `__pow__` method in `Tensor` for this op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"op_power_scalar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"op_power_scalar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LogSoftmax\n",
    "\n",
    "LogSoftmax is useful for implementing softmax loss since we can write its forward and backward pass in terms of it. It is simply the log of the softmax function:\n",
    "\n",
    "$$\n",
    "\\mathsf{LogSoftmax}(x) = \\log\\left(\\frac{\\exp(x)}{\\sum_i \\exp(x_i)}\\right)\n",
    "$$\n",
    "\n",
    "Consider how you could simplify this expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"op_logsoftmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"op_logsoftmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1 [5 points]\n",
    "\n",
    "Before we can implement certain neural network library modules, we need an assortment\n",
    "of methods to initialize tensors from various distributions.\n",
    "In this question, you will implement different methods for weight initialization. Specifically, in `python/needle/init.py` implement the functions:\n",
    "___\n",
    "### Uniform\n",
    "`needle.init.uniform(x, low=0.0, high=1.0)`\n",
    "\n",
    "Fills the input Tensor with values drawn from the uniform distribution $\\mathcal{U}(a,b)$.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `low` - lower bound of the uniform distribution\n",
    "- `high` - upper bound of the uniform distribution\n",
    "___\n",
    "\n",
    "### Normal\n",
    "`needle.init.normal(x, mean=0.0, std=1.0)`\n",
    "\n",
    "Fills the input Tensor with values drawn from the normal distribution $\\mathcal{N}(\\text{mean},\\text{std}^2)$.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `mean` - mean of the normal distribution\n",
    "- `std` - standard deviation of the normal distribution\n",
    "___\n",
    "\n",
    "### Constant\n",
    "`needle.init.constant(x, c=0.0)`\n",
    "\n",
    "Fills the input Tensor with value `c`.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `c` - the value to fill the Tensor with\n",
    "___\n",
    "\n",
    "### Ones\n",
    "`needle.init.ones(x, c=0.0)`\n",
    "\n",
    "Fills the input Tensor with scalar value 1.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "___\n",
    "\n",
    "### Zeros\n",
    "`needle.init.zeros(x)`\n",
    "\n",
    "Fills the input Tensor with scalar value 0.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "___\n",
    "\n",
    "### Xavier uniform\n",
    "`needle.init.xavier_uniform(x, gain=1.0)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-a, a)$ where \n",
    "\\begin{equation}\n",
    "a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Xavier normal\n",
    "`needle.init.xavier_normal(x, gain=1.0)`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan_in} + \\text{fan_out}}}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `gain` - optional scaling factor\n",
    "___\n",
    "\n",
    "### Kaiming uniform\n",
    "`needle.init.kaiming_uniform(x, mode='fan_in', nonlinearity='relu')`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{U}(-\\text{bound}, \\text{bound})$ where \n",
    "\\begin{equation}\n",
    "\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan_mode}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `mode` - either `fan_in` or `fan_out`. Choosing `fan_in` preserves the magnitude of the variance of the weights in the forward pass. Choosing `fan_out` preserves the magnitudes in the backwards pass. \n",
    "- `nonlinearity` - the non-linear function\n",
    "___\n",
    "\n",
    "### Kaiming normal\n",
    "`needle.init.kaiming_normal(x, mode='fan_in', nonlinearity='relu')`\n",
    "\n",
    "Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\\mathcal{N}(0, \\text{std}^2)$ where \n",
    "\\begin{equation}\n",
    "\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan_mode}}}\n",
    "\\end{equation}\n",
    "\n",
    "Use the recommended gain value for ReLU: $\\text{gain}=\\sqrt{2}$.\n",
    "\n",
    "##### Parameters\n",
    "- `x` - Tensor\n",
    "- `mode` - either `fan_in` or `fan_out`. Choosing `fan_in` preserves the magnitude of the variance of the weights in the forward pass. Choosing `fan_out` preserves the magnitudes in the backwards pass. \n",
    "- `nonlinearity` - the non-linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_init\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"init\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 [30 points]\n",
    "\n",
    "In this question, you will implement additional modules in `python/needle/nn.py`. Specifically, for the following modules described below, initialize any variables of the module in the constructor, and fill out the `forward` method. \n",
    "___\n",
    "\n",
    "### Linear\n",
    "`needle.nn.Linear(in_features, out_features, bias=True, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies a linear transformation to the incoming data: $y = xA^T + b$. The input shape is $(N, *, H_{in})$ where * means any number of additional dimensions and $H_{in}=\\text{in_features}$. The output shape is $(N, *, H_{out})$ where all but the last dimension are the same shape as the input and $H_{out}=\\text{out_features}$.\n",
    "\n",
    "Be careful to explicitly broadcast the bias term to the correct shape -- Needle does not support implicit broadcasting.\n",
    "\n",
    "\n",
    "##### Parameters\n",
    "- `in_features` - size of each input sample\n",
    "- `out_features` - size of each output sample\n",
    "- `bias` - If set to `False`, the layer will not learn an additive bias.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of shape (`in_features`, `out_features`). The values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{in_features}}$.\n",
    "- `bias` - the learnable bias of shape (`out_features`). If `bias` is `True`, the values are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k=\\frac{1}{\\text{in_features}}$.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "`needle.nn.ReLU(device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies the rectified linear unit function element-wise:\n",
    "$ReLU(x) = max(0, x)$.\n",
    "\n",
    "If you have previously implemented ReLU's backwards pass in terms of itself, note that this is numerically unstable and will likely cause problems\n",
    "down the line.\n",
    "Instead, consider that we could write the derivative of ReLU as $I\\{x>0\\}$, where we arbitrarily decide that the derivative at $x=0$ is 0.\n",
    "(This is a _subdifferentiable_ function.)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_relu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sequential\n",
    "`needle.nn.Sequential(*modules, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies a sequence of modules to the input (in the order that they were passed to the constructor) and returns the output of the last module.\n",
    "These should be kept in a `.module` property: you should _not_ redefine any magic methods like `__getitem__`, as this may not be compatible with our tests.\n",
    "\n",
    "##### Parameters\n",
    "- `*modules` - any number of modules of type `needle.nn.Module`\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_sequential\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_sequential\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SoftmaxLoss\n",
    "`needle.nn.SoftmaxLoss(device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies the softmax loss as defined below (and as implemented in Homework 1), taking in as input a Tensor of logits and a Tensor of the true labels (expressed as a list of numbers, *not* one-hot encoded).\n",
    "\n",
    "Note that you can use the new `ops.one_hot` function now instead of writing this yourself.\n",
    "**Importantly**, you should implement your SoftmaxLoss in terms of the `LogSoftmaxOp` you implemented in Q0.\n",
    "Also note that the equation below is equal to negative log softmax.\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_\\text{softmax}(z,y) = \\log \\sum_{i=1}^k \\exp z_i - z_y\n",
    "\\end{equation}\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_softmax_loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### LayerNorm\n",
    "`needle.nn.LayerNorm(dims, eps=1e-5, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies layer normalization over a mini-batch of inputs as described in the paper [Layer Normalization](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{z}_{i+1} = \\sigma_i(W_i^Tz_i + b_i) \\\\\n",
    "z_{i+1} = \\frac{\\hat{z}_{i+1} - \\text{E}[\\hat{z}_{i+1}]}{(\\text{Var}[\\hat{z}_{i+1}]+\\epsilon)^{1/2})}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the affine scaling parameters `weight` and `bias` here are shaped differently than for BatchNorm below.\n",
    "\n",
    "##### Parameters\n",
    "- `dims` - input shape\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of shape `dims`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of shape `dims`, elements initialized to 0.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_layernorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_layernorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm\n",
    "`needle.nn.BatchNorm(dim, eps=1e-5, momentum=0.1, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies batch normalization over a mini-batch of inputs as described in the paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167). Unlike layer normalization, normalizes activations over the mini-batch rather than normalizing the activations at each layer. Computes a running average of mean/variance for all features at each layer $\\hat{\\mu}_{i+1}, \\hat{\\sigma}^2_{i+1}$, and at test time normalizes by these quantities:\n",
    "\n",
    "\\begin{equation}\n",
    "(z_{i+1})_j = \\frac{(\\hat{z}_{i+1})_j - (\\hat{\\mu}_{i+1})_j}{((\\hat{\\sigma}^2_{i+1})_j+\\epsilon)^{1/2}}\n",
    "\\end{equation}\n",
    "\n",
    "You will additionally have to write the function `_child_modules` in `nn.py` in order to make\n",
    "it possible to set a flag on a module and its children which indicates whether or not training is in progress;\n",
    "that is, all modules have a property `training` which should be false after calling `model.eval()` on the module or one of its parents, and vice versa for `model.train()`. (There is a test for this in this section.)\n",
    "\n",
    "BatchNorm uses the running estimates of mean and variance instead of batch statistics at test time, i.e.,\n",
    "after `model.eval()` has been called on the BatchNorm layer's `training` flag is false.\n",
    "\n",
    "**Important:** A small detail here is that our implementation of BatchNorm uses the *biased* estimate\n",
    "of the variance during training, but computes a running estimate of the *unbiased* version. The biased estimate\n",
    "divides by $N$, while the unbiased estimate divides by $N-1$.\n",
    "\n",
    "To compute the running estimates, you can use the equation $$x_{new}' = (1 - m) x_{old}' + m x_{observed},$$\n",
    "where $m$ is momentum.\n",
    "\n",
    "##### Parameters\n",
    "- `dim` - input dim\n",
    "- `eps` - a value added to the denominator for numerical stability.\n",
    "- `momentum` - the value used for the running mean and running variance computation.\n",
    "\n",
    "##### Variables\n",
    "- `weight` - the learnable weights of size `dim`, elements initialized to 1.\n",
    "- `bias` - the learnable bias of size `dim`, elements initialized to 0.\n",
    "- `running_mean` - the running mean used at evaluation time\n",
    "- `running_var` - the running (unbiased) variance used at evaluation time. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_batchnorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_batchnorm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "`needle.nn.Dropout(drop_prob, device=None, dtype=\"float32\")`\n",
    "\n",
    "During training, randomly zeroes some of the elements of the input tensor with probability `drop_prob` using samples from a Bernoulli distribution. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper [Improving neural networks by preventing co-adaption of feature detectors](https://arxiv.org/abs/1207.0580). During evaluation the module simply computes an identity function. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{z}_{i+1} = \\sigma_i (W_i^T z_i + b_i) \\\\\n",
    "(z_{i+1})_j = \n",
    "    \\begin{cases}\n",
    "    (\\hat{z}_{i+1})_j /(1-p) & \\text{with probability } 1-p \\\\\n",
    "    0 & \\text{with probability } p \\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "**Important**: If the Dropout module the flag `training=False`, you shouldn't \"dropout\" any weights. That is, dropout applies during training only, not during evaluation.\n",
    "\n",
    "##### Parameters\n",
    "- `drop_prob` - probability of an element to be zeroed. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_dropout\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Residual\n",
    "`needle.nn.Residual(fn: Module, device=None, dtype=\"float32\")`\n",
    "\n",
    "Applies a residual or skip connection given module $\\mathcal{F}$ and input Tensor $x$, returning $\\mathcal{F}(x) + x$.\n",
    "##### Parameters\n",
    "- `fn` - module of type `needle.nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_nn_residual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_residual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3 [30 points]\n",
    "\n",
    "Implement the `step` function of the following optimizers.\n",
    "Make sure that your optimizers _don't_ modify the gradients of tensors in-place.\n",
    "\n",
    "We have included some tests to ensure that you are not consuming excessive memory, which can happen if you are\n",
    "not using `.data` or `.detach()` in the right places, thus building an increasingly large computational graph\n",
    "(not just in the optimizers, but in the previous modules as well).\n",
    "You can ignore these tests, which include the string `check_memory` at your own discretion.\n",
    "\n",
    "___\n",
    "\n",
    "### SGD\n",
    "`needle.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.0)`\n",
    "\n",
    "Implements stochastic gradient descent (optionally with momentum, shown as $\\beta$ below). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    u_{t+1} &= \\beta u_t + \\nabla_\\theta f(\\theta_t) \\\\\n",
    "    \\theta_{t+1} &= \\theta_t - \\alpha u_{t+1}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `momentum` (*float*) - momentum factor\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_sgd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"optim_sgd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adam\n",
    "`needle.optim.Adam(params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0)`\n",
    "\n",
    "Implements Adam algorithm, proposed in [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980). \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "u_{t+1} &= \\beta_1 u_t + (1-\\beta_1) \\nabla_\\theta f(\\theta_t) \\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2) (\\nabla_\\theta f(\\theta_t))^2 \\\\\n",
    "\\hat{u_{t+1}} &= u_{t+1} / (1 - \\beta_1^t) \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{v_{t+1}} &= v_{t+1} / (1 - \\beta_2^t) \\quad \\text{(bias correction)}\\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\alpha \\hat{u_{t+1}}/(\\hat{v_{t+1}}^{1/2}+\\epsilon)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "**Important:** Pay attention to whether or not you are applying bias correction.\n",
    "\n",
    "##### Parameters\n",
    "- `params` - iterable of parameters of type `needle.nn.Parameter` to optimize\n",
    "- `lr` (*float*) - learning rate\n",
    "- `beta1` (*float*) - coefficient used for computing running average of gradient\n",
    "- `beta2` (*float*) - coefficient used for computing running average of square of gradient\n",
    "- `eps` (*float*) - term added to the denominator to improve numerical stability\n",
    "- `bias_correction` - whether to use bias correction for $u, v$\n",
    "- `weight_decay` (*float*) - weight decay (L2 penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_optim_adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"optim_adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 [10 points]\n",
    "\n",
    "In this question, you will implement two data primitives: `needle.data.DataLoader` and `needle.data.Dataset`. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples. \n",
    "\n",
    "For this question, you will be working in `python/needle/data.py`. First, copy your solution to `parse_mnist` from the previous homework into the `parse_mnist` function. \n",
    "\n",
    "### Transformations\n",
    "\n",
    "First we will implement a few transformations that are helpful when working with images. We will stick with a horizontal flip and a random crop for now. Fill out the following functions in `data.py`.\n",
    "___ \n",
    "\n",
    "#### FlipHorizontal\n",
    "`needle.data.FlipHorizontal()`\n",
    "\n",
    "Flips the image horizontally.\n",
    "___\n",
    "\n",
    "#### RandomCrop\n",
    "`needle.data.RandomCrop(padding=3)`\n",
    "\n",
    "Padding is added to all side of the image, and then the image is cropped back to it's original size at a random location. Returns an image the same size as the original image.\n",
    "\n",
    "##### Parameters\n",
    "- `padding` (*int*) - The padding on each border of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"flip_horizontal\"\n",
    "!python3 -m pytest -v -k \"random_crop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"flip_horizontal\"\n",
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"random_crop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Each `Dataset` subclass must implement three functions: `__init__`, `__len__`, and `__getitem__`. The `__init__` function initializes the images, labels, and transforms. The `__len__` function returns the number of samples in the dataset. The `__getitem__` function retrieves a sample from the dataset at a given index `idx`, calls the transform functions on the image (if applicable), converts the image and label to a numpy array (the data will be converted to Tensors elsewhere). Fill out these functions in the `MNISTDataset` class: \n",
    "___ \n",
    "\n",
    "### MNISTDataset\n",
    "`needle.data.MNISTDataset(image_filesname, label_filesname)`\n",
    "\n",
    "##### Parameters\n",
    "- `image_filesname` - path of file containing images\n",
    "- `label_filesname` - path of file containing labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"mnist_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"mnist_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling \n",
    "\n",
    "During training, we typically want to pass samples in mini-batches, and shuffle the data at each epoch to reduce model overfitting. The dataloader we will eventually build will be flexible enough to change how it samples points from a given dataset, as well as how it combines the data at a batch level. First, we define a `Sampler` class that will be subclassed to build different data sampling strategies. In this homework we will extend this and create a `SequentialSampler` and a `RandomSampler`. Furthermore, we typically batch our data. Therefore, we will also construct a `BatchSampler`, which will take a `Sampler` and return the sampled indexes as batches instead of individual indexes. Each iteration of a sampler will return an index, whereas each iteration of a batch sampler will return a list of indexes. Fill out the following classes in `python/needle/data.py`.\n",
    "___\n",
    "\n",
    "### SequentialSampler\n",
    "`needle.data.SequentialSampler(data_source: needle.data.Dataset)`\n",
    "\n",
    "Samples elements sequentially, always in the same order. \n",
    "\n",
    "##### Parameters\n",
    "- `data_source` - `needle.data.Dataset` - dataset \n",
    "___ \n",
    "\n",
    "### RandomSampler\n",
    "`needle.data.RandomSampler(data_source: Sized, replacement: bool = False,\n",
    "                 num_samples: Optional[int] = None)`\n",
    "\n",
    "Samples elements randomly. If replacement is specified then you must also specify `num_samples`. If replacement is not specified, then the `data_source` size will be used as `num_samples` and all indices shuffled. \n",
    "\n",
    "##### Parameters\n",
    "- `data_source` - `needle.data.Dataset` - dataset \n",
    "- `replacement` - `bool` - whether or not to use replacement when sampling from the dataset\n",
    "- `num_samples` - `int` - if using replacement, how many samples to draw\n",
    "___ \n",
    "\n",
    "### BatchSampler\n",
    "`needle.data.BatchSampler(sampler: Union[Sampler, Iterable], batch_size: int, drop_last: bool)`\n",
    "\n",
    "Given a sampler, batch the sampled indexes in sets of `batch_size`. If `drop_last` is set to `True`, and the final batch is not full, then it is dropped. \n",
    "\n",
    "##### Parameters\n",
    "- `sampler` - `needle.data.Sampler` - the sampler who's output is to be batched\n",
    "- `batch_size` - `int` - the batch size, or the set size to group the indexes in\n",
    "- `drop_last` - `bool` - if set, the final batch is dropped if it is less than `batch_size`\n",
    "___ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"sequential_sampler\"\n",
    "!python3 -m pytest -v -k \"random_sampler\"\n",
    "!python3 -m pytest -v -k \"batch_sampler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"sequential_sampler\"\n",
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"random_sampler\"\n",
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"batch_sampler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating\n",
    "\n",
    "So far we have a `Dataset`, a `Sampler` to draw data points from the dataset, and we can even batch these data points by wrapping our `Sampler` in a `BatchSampler`. However, there is a final step we need to perform before serving data points: collation. Our data is not necessarily in the exact format or structure that we want to leverage during training, and these specifications can differ. Here we will write a collator function for the MNIST dataset. This function takes a batch of data and refactors it such that we return the input data and label separately, the outer dimension of these returned variables is the batch size, and they are of the proper data type (`Tensor` (*float64*)). Fill out the following function in `data.py`.\n",
    "___\n",
    "#### collate_mnist\n",
    "`needle.data.collate_mnist(batch)`\n",
    "\n",
    "Take a batch of data and transform it into a Tensor of the proper shape.\n",
    "\n",
    "##### Parameters\n",
    "- `batch` - a batch of data, can be of different types including a tuple or a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"collate_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"collate_mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "We now have all of the components we need to create a dataloader. Using all of the prior objects and methods created in this question, we will build a dataloader object to combine them together. We have provided a base `Dataloader` implementation that is flexible enough to include multiprocess loading of samples (without multithreading, data loading can become the bottle neck when using fast computation methods). For simplicity we will only implement a single threaded dataloader, but we hope that for those interested in how to extend this to a multi-threaded process, the \"jump-off\" points are clear. This also applies to the underlying data type. In this homework we focus on `Iterable` datasets, but commonly `Map` style datasets are used as well. Again, we hope that the base implemntation makes it clear how one would extend this to work with `Map` style datasets, although we only work with `Iterable` ones here. Look through the `DataLoader` class, as well as the classes it calls/depends on. Then fill out the `fetch` method of `_IterableDatasetFetcher`. \n",
    "___\n",
    "\n",
    "### Dataloader\n",
    "`needle.data.Dataloader(dataset: Dataset, batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Union[Sampler, Iterable, None] = None, collate_fn: Optional = default_collate, drop_last: bool = False,`\n",
    "\n",
    "Combine a dataset, sampler, and collator to serve datapoints from a dataset. \n",
    "\n",
    "##### Parameters\n",
    "- `dataset` - `needle.data.Dataset` - a dataset \n",
    "- `batch_size` - `int` - what batch size to serve the data in \n",
    "- `shuffle` - `bool` - if a sampler is not provided, use this to choose between a `SequentialSampler` and a `RandomSampler`\n",
    "- `collate_fn` - `Generic` - the function to use to collate the samples. For simplicity we will be using only the collate_mnist here, but a general collation function can be built. \n",
    "- `drop_last` - `bool` - whether or not to drop the last batch if it is not full. \n",
    "___ \n",
    "\n",
    "### \\_IterableDatasetFetcher\n",
    "`needle.data._IterableDatasetFetcher(dataset, collate_fn, drop_last)`\n",
    "\n",
    "This function takes a batch of indexes (typically returned from a `Sampler`), acquires the data from the `dataset`, collates the data using the `collate_fn`, and then returns it for use.  \n",
    "\n",
    "##### Parameters\n",
    "- `dataset` - `needle.data.Dataset` - a dataset \n",
    "- `collate_fn` - `Generic` - the function to use to collate the samples. For simplicity we will be using only the `collate_mnist` here, but a general collation function can be built. \n",
    "- `drop_last` - `bool` - whether or not to drop the last batch if it is not full. \n",
    "___ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_dataloader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"dataloader\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 [20 points]\n",
    "\n",
    "Given you have now implemented all the necessary components for our neural network library, let's build and train an MLP ResNet. For this question, you will be working in `apps/mlp_resnet.py`. First, fill out the functions `ResidualBlock` and `MLPResNet` as described below:\n",
    "\n",
    "### ResidualBlock\n",
    "`ResidualBlock(dim, hidden_dim, norm=nn.BatchNorm, drop_prob=0.9)`\n",
    "\n",
    "Implements a residual block as follows:\n",
    "![](figures/residualblock.png)\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and the last linear layer has `out_features=dim`. Returns the block as type `nn.Module`. \n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `norm` (*nn.Module*) - normalization method\n",
    "- `drop_prob` (*float*) - dropout probability\n",
    "\n",
    "___\n",
    "\n",
    "### MLPResNet\n",
    "`ResidualBlock(dim, hidden_dim=100, num_blocks=3, num_classes=10, norm=nn.BatchNorm, drop_prob=0.9)`\n",
    "\n",
    "Implements an MLP ResNet as follows:\n",
    "![](figures/mlp_resnet.png)\n",
    "where the first linear layer has `in_features=dim` and `out_features=hidden_dim`, and each ResidualBlock has `dim=hidden_dim` and `hidden_dim=hidden_dim//2`. Returns a network of type `nn.Module`. \n",
    "\n",
    "##### Parameters\n",
    "- `dim` (*int*) - input dim\n",
    "- `hidden_dim` (*int*) - hidden dim\n",
    "- `num_blocks` (*int*) - number of ResidualBlocks\n",
    "- `num_classes` (*int*) - number of classes\n",
    "___ \n",
    "\n",
    "Once you have the deep learning model architecture correct, let's train the network using our new neural network library components. Specifically, implement the functions `train_epoch`, `evaluate` and `train_mnist`. \n",
    "\n",
    "___\n",
    "`train_epoch(dataloader, model, loss_fn, opt)`\n",
    "\n",
    "Executes one epoch of training, iterating over the entire training dataset once (just like `nn_epoch` from previous homeworks). Returns the average accuracy (as a `float`) and the average loss over all samples (as a `Tensor`, detached from the computation graph). Sets the model to `training` mode at the beginning of the function.  \n",
    "\n",
    "##### Parameters\n",
    "- `dataloader` (*`needle.data.DataLoader`*) - dataloader returning samples from the training dataset\n",
    "- `model` (*`needle.nn.Module`*) - neural network\n",
    "- `loss_fn` (*`needle.nn.Module` type*) - loss function to optimize over\n",
    "- `opt` (*`needle.optim.Optimizer`*) - optimizer instance\n",
    "\n",
    "___\n",
    "`evaluate(dataloader, model, loss_fn)`\n",
    "\n",
    "Evaluates the model given a loss function on the entire test dataset. Returns the average accuracy (as a `float`) and the average loss over all samples (as a `Tensor`). Sets the model to `eval` mode at the beginning of the function. \n",
    "\n",
    "##### Parameters\n",
    "- `dataloader` (*`needle.data.DataLoader`*) - dataloader returning samples from the test dataset\n",
    "- `model` (*`needle.nn.Module`*) - neural network \n",
    "- `loss_fn` (*`needle.nn.Module` type*) - loss function\n",
    "___\n",
    "\n",
    "`train_mnist(batch_size=100, epochs=10, optimizer=ndl.optim.Adam, \n",
    "                lr=0.001, weight_decay=0.001, hidden_dim=100, data_dir=\"data\")`\n",
    "                \n",
    "Initializes a training dataloader (with `shuffle` set to `True`) and a test dataloader for MNIST data, and trains an `MLPResNet` using the given optimizer and the softmax loss for a given number of epochs. Returns a tuple of the training accuracy, training loss, test accuracy, test loss computed in the last epoch of training. If any parameters are not specified, use the default parameters.\n",
    "\n",
    "##### Parameters\n",
    "- `batch_size` (*int*) - batch size to use for train and test dataloader\n",
    "- `epochs` (*int*) - number of epochs to train for\n",
    "- `optimizer` (*`needle.optim.Optimizer` type*) - optimizer type to use\n",
    "- `lr` (*float*) - learning rate \n",
    "- `weight_decay` (*float*) - weight decay\n",
    "- `hidden_dim` (*int*) - hidden dim for `MLPResNet`\n",
    "- `data_dir` (*int*) - directory containing MNIST image/label files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pytest -v -k \"test_mlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"mlp_resnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage to experiment with the `mlp_resnet.py` training script.\n",
    "You can uncomment the lines that print training statistics and investigate\n",
    "the effect of using different initializers on the Linear layers,\n",
    "increasing the dropout probability,\n",
    "or adding transforms (via a list to the `transforms=` keyword argument of Dataset)\n",
    "such as random cropping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
