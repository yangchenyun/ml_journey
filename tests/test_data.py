import sys
sys.path.append("./python")
import needle as ndl
import numpy as np

import mugrade


def test_flip_horizontal():
    tform = ndl.data.FlipHorizontal()

    np.random.seed(0)
    a = np.array([0.5928446182250183, 0.8442657485810173, 0.8579456176227568, 0.8472517387841254, 0.6235636967859723,
                  0.3843817072926998, 0.2975346065444723, 0.05671297731744318, 0.2726562945801132, 0.47766511732134986,
                  0.8121687287754932, 0.4799771723750573, 0.3927847961008297, 0.8360787635373775, 0.3373961604172684,
                  0.6481718720511972, 0.36824153984054797, 0.9571551589530464, 0.14035078041264515, 0.8700872583584364,
                  0.4736080452737105, 0.8009107519796442, 0.5204774795512048, 0.6788795301189603, 0.7206326547259168,
                  0.5820197920751071, 0.5373732294490107, 0.7586156243223572, 0.10590760718779213, 0.4736004193466574,
                  0.18633234332675996, 0.7369181771289581, 0.21655035442437187, 0.13521817340545206, 0.3241410077932141,
                  0.14967486718368317])
    b = np.array([0.3843817072926998, 0.6235636967859723, 0.8472517387841254, 0.8579456176227568, 0.8442657485810173,
                  0.5928446182250183, 0.4799771723750573, 0.8121687287754932, 0.47766511732134986, 0.2726562945801132,
                  0.05671297731744318, 0.2975346065444723, 0.9571551589530464, 0.36824153984054797, 0.6481718720511972,
                  0.3373961604172684, 0.8360787635373775, 0.3927847961008297, 0.6788795301189603, 0.5204774795512048,
                  0.8009107519796442, 0.4736080452737105, 0.8700872583584364, 0.14035078041264515, 0.4736004193466574,
                  0.10590760718779213, 0.7586156243223572, 0.5373732294490107, 0.5820197920751071, 0.7206326547259168,
                  0.14967486718368317, 0.3241410077932141, 0.13521817340545206, 0.21655035442437187, 0.7369181771289581,
                  0.18633234332675996])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.6169339968747569, 0.9437480785146242, 0.6818202991034834, 0.359507900573786])
    b = np.array([0.9437480785146242, 0.6169339968747569, 0.359507900573786, 0.6818202991034834])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.9023485831739843, 0.09928035035897387, 0.9698090677467488, 0.6531400357979377])
    b = np.array([0.09928035035897387, 0.9023485831739843, 0.6531400357979377, 0.9698090677467488])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.2103825610738409, 0.1289262976548533, 0.31542835092418386, 0.3637107709426226, 0.5701967704178796,
                  0.43860151346232035, 0.9883738380592262, 0.10204481074802807, 0.2088767560948347, 0.16130951788499626,
                  0.6531083254653984, 0.2532916025397821, 0.4663107728563063, 0.24442559200160274, 0.15896958364551972,
                  0.11037514116430513, 0.6563295894652734, 0.1381829513486138, 0.1965823616800535, 0.3687251706609641,
                  0.8209932298479351, 0.09710127579306127, 0.8379449074988039, 0.09609840789396307, 0.9764594650133958])
    b = np.array([0.5701967704178796, 0.3637107709426226, 0.31542835092418386, 0.1289262976548533, 0.2103825610738409,
                  0.16130951788499626, 0.2088767560948347, 0.10204481074802807, 0.9883738380592262, 0.43860151346232035,
                  0.15896958364551972, 0.24442559200160274, 0.4663107728563063, 0.2532916025397821, 0.6531083254653984,
                  0.3687251706609641, 0.1965823616800535, 0.1381829513486138, 0.6563295894652734, 0.11037514116430513,
                  0.9764594650133958, 0.09609840789396307, 0.8379449074988039, 0.09710127579306127, 0.8209932298479351])
    np.testing.assert_allclose(tform(a), b)

    a = np.array(
        [0.5096243767199001, 0.05571469370160631, 0.4511592145209281, 0.019987665408758737, 0.44171092124884537,
         0.9795867288127285, 0.3594444639693215, 0.4808935308361628, 0.6886611828057704, 0.8804758892525955,
         0.9182354663621447, 0.21682213762754288, 0.5651888666048753, 0.865102561305385, 0.5089689606670145,
         0.9167229540194608])
    b = np.array([0.019987665408758737, 0.4511592145209281, 0.05571469370160631, 0.5096243767199001, 0.4808935308361628,
                  0.3594444639693215, 0.9795867288127285, 0.44171092124884537, 0.21682213762754288, 0.9182354663621447,
                  0.8804758892525955, 0.6886611828057704, 0.9167229540194608, 0.5089689606670145, 0.865102561305385,
                  0.5651888666048753])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.5759464955561793, 0.9292961975762141, 0.31856895245132366, 0.6674103799636817, 0.13179786240439217,
                  0.7163272041185655, 0.2894060929472011, 0.18319136200711683, 0.5865129348100832, 0.020107546187493552,
                  0.8289400292173631, 0.004695476192547066, 0.6778165367962301, 0.27000797319216485, 0.7351940221225949,
                  0.9621885451174382])
    b = np.array([0.6674103799636817, 0.31856895245132366, 0.9292961975762141, 0.5759464955561793, 0.18319136200711683,
                  0.2894060929472011, 0.7163272041185655, 0.13179786240439217, 0.004695476192547066, 0.8289400292173631,
                  0.020107546187493552, 0.5865129348100832, 0.9621885451174382, 0.7351940221225949, 0.27000797319216485,
                  0.6778165367962301])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.7561066938650409, 0.3960982754233623, 0.896038387509077, 0.6389210762313129])
    b = np.array([0.3960982754233623, 0.7561066938650409, 0.6389210762313129, 0.896038387509077])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.952749011516985, 0.44712537861762736, 0.8464086724711278, 0.6994792753175043, 0.29743695085513366,
                  0.8137978197024772, 0.39650574084698464, 0.8811031971111616, 0.5812728726358587, 0.8817353618548528,
                  0.6925315900777659, 0.7252542798196405, 0.5013243819267023, 0.9560836347232239, 0.6439901992296374,
                  0.4238550485581797, 0.6063932141279244, 0.019193198309333526, 0.30157481667454933, 0.660173537492685,
                  0.29007760721044407, 0.6180154289988415, 0.42876870094576613, 0.13547406422245023,
                  0.29828232595603077])
    b = np.array([0.29743695085513366, 0.6994792753175043, 0.8464086724711278, 0.44712537861762736, 0.952749011516985,
                  0.8817353618548528, 0.5812728726358587, 0.8811031971111616, 0.39650574084698464, 0.8137978197024772,
                  0.6439901992296374, 0.9560836347232239, 0.5013243819267023, 0.7252542798196405, 0.6925315900777659,
                  0.660173537492685, 0.30157481667454933, 0.019193198309333526, 0.6063932141279244, 0.4238550485581797,
                  0.29828232595603077, 0.13547406422245023, 0.42876870094576613, 0.6180154289988415,
                  0.29007760721044407])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.5518162591332588, 0.033625093498832026, 0.9689617652602737, 0.3209972413734975, 0.22126268518378278,
                  0.1412639049258967, 0.09725992706317388, 0.9840422413641464, 0.2603409275874047])
    b = np.array([0.9689617652602737, 0.033625093498832026, 0.5518162591332588, 0.1412639049258967, 0.22126268518378278,
                  0.3209972413734975, 0.2603409275874047, 0.9840422413641464, 0.09725992706317388])
    np.testing.assert_allclose(tform(a), b)

    a = np.array([0.10022688731230112, 0.9194826137446735, 0.7142412995491114, 0.9988470065678665, 0.14944830465799375,
                  0.8681260573682142, 0.16249293467637482, 0.6155595642838442, 0.12381998284944151, 0.8480082293222344,
                  0.8073189587250107, 0.5691007386145933, 0.40718329722599966, 0.06916699545513805, 0.6974287731445636,
                  0.45354268267806885, 0.7220555994703479, 0.8663823259286292, 0.9755215050028858, 0.855803342392611,
                  0.011714084185001972, 0.3599780644783639, 0.729990562424058, 0.17162967726144052, 0.5210366062041293])
    b = np.array([0.14944830465799375, 0.9988470065678665, 0.7142412995491114, 0.9194826137446735, 0.10022688731230112,
                  0.8480082293222344, 0.12381998284944151, 0.6155595642838442, 0.16249293467637482, 0.8681260573682142,
                  0.6974287731445636, 0.06916699545513805, 0.40718329722599966, 0.5691007386145933, 0.8073189587250107,
                  0.855803342392611, 0.9755215050028858, 0.8663823259286292, 0.7220555994703479, 0.45354268267806885,
                  0.5210366062041293, 0.17162967726144052, 0.729990562424058, 0.3599780644783639, 0.011714084185001972])
    np.testing.assert_allclose(tform(a), b)

    # np.random.seed(0)
    # print('np.random.seed(0)')
    # for _ in range(10):
    #     size = np.random.randint(2, 7)
    #     a = np.random.rand(size, size).flatten()
    #     b = tform(a)
    #     print(f'a = np.array({a.tolist()})')
    #     print(f'b = np.array({b.tolist()})')
    #     print(f'np.testing.assert_allclose(tform(a), b)')
    #     print()



# ----------------------------------------------------------------------------
def submit_flip_horizontal():
    tform = ndl.data.FlipHorizontal()
    np.random.seed(0)
    for _ in range(10):
        side = np.random.randint(1, 5)
        mugrade.submit(tform(np.random.rand(side, side).flatten()))


def test_random_crop():
    tform = ndl.data.RandomCrop(1)
    a = np.array([0.41097298955548633, 0.35635812630859065, 0.1924875960135809, 0.7950153315610718])
    b = np.array([0.0, 0.0, 0.41097298955548633, 0.35635812630859065])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(6)
    a = np.array([0.6027633760716439])
    b = np.array([0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(4)
    a = np.array([0.5448831829968969, 0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798,
                  0.9636627605010293, 0.3834415188257777, 0.7917250380826646, 0.5288949197529045, 0.5680445610939323,
                  0.925596638292661, 0.07103605819788694, 0.08712929970154071, 0.02021839744032572, 0.832619845547938,
                  0.7781567509498505])
    b = np.array(
        [0.4375872112626925, 0.0, 0.0, 0.0, 0.7917250380826646, 0.0, 0.0, 0.0, 0.07103605819788694, 0.0, 0.0, 0.0,
         0.7781567509498505, 0.0, 0.0, 0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(6)
    a = np.array([0.6027633760716439])
    b = np.array([0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(4)
    a = np.array([0.5448831829968969, 0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798,
                  0.9636627605010293, 0.3834415188257777, 0.7917250380826646, 0.5288949197529045, 0.5680445610939323,
                  0.925596638292661, 0.07103605819788694, 0.08712929970154071, 0.02021839744032572, 0.832619845547938,
                  0.7781567509498505])
    b = np.array(
        [0.4375872112626925, 0.0, 0.0, 0.0, 0.7917250380826646, 0.0, 0.0, 0.0, 0.07103605819788694, 0.0, 0.0, 0.0,
         0.7781567509498505, 0.0, 0.0, 0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(6)
    a = np.array([0.6027633760716439])
    b = np.array([0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(4)
    a = np.array([0.5448831829968969, 0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798,
                  0.9636627605010293, 0.3834415188257777, 0.7917250380826646, 0.5288949197529045, 0.5680445610939323,
                  0.925596638292661, 0.07103605819788694, 0.08712929970154071, 0.02021839744032572, 0.832619845547938,
                  0.7781567509498505])
    b = np.array(
        [0.4375872112626925, 0.0, 0.0, 0.0, 0.7917250380826646, 0.0, 0.0, 0.0, 0.07103605819788694, 0.0, 0.0, 0.0,
         0.7781567509498505, 0.0, 0.0, 0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(6)
    a = np.array([0.6027633760716439])
    b = np.array([0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(4)
    a = np.array([0.5448831829968969, 0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798,
                  0.9636627605010293, 0.3834415188257777, 0.7917250380826646, 0.5288949197529045, 0.5680445610939323,
                  0.925596638292661, 0.07103605819788694, 0.08712929970154071, 0.02021839744032572, 0.832619845547938,
                  0.7781567509498505])
    b = np.array(
        [0.4375872112626925, 0.0, 0.0, 0.0, 0.7917250380826646, 0.0, 0.0, 0.0, 0.07103605819788694, 0.0, 0.0, 0.0,
         0.7781567509498505, 0.0, 0.0, 0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)

    tform = ndl.data.RandomCrop(4)
    a = np.array([0.7151893663724195, 0.6027633760716439, 0.5448831829968969, 0.4236547993389047, 0.6458941130666561,
                  0.4375872112626925, 0.8917730007820798, 0.9636627605010293, 0.3834415188257777, 0.7917250380826646,
                  0.5288949197529045, 0.5680445610939323, 0.925596638292661, 0.07103605819788694, 0.08712929970154071,
                  0.02021839744032572, 0.832619845547938, 0.7781567509498505, 0.8700121482468192, 0.978618342232764,
                  0.7991585642167236, 0.46147936225293185, 0.7805291762864555, 0.11827442586893322, 0.6399210213275238,
                  0.1433532874090464, 0.9446689170495839, 0.5218483217500717, 0.4146619399905236, 0.26455561210462697,
                  0.7742336894342167, 0.45615033221654855, 0.5684339488686485, 0.018789800436355142, 0.6176354970758771,
                  0.6120957227224214, 0.6169339968747569, 0.9437480785146242, 0.6818202991034834, 0.359507900573786,
                  0.43703195379934145, 0.6976311959272649, 0.06022547162926983, 0.6667667154456677, 0.6706378696181594,
                  0.2103825610738409, 0.1289262976548533, 0.31542835092418386, 0.3637107709426226, 0.5701967704178796,
                  0.43860151346232035, 0.9883738380592262, 0.10204481074802807, 0.2088767560948347, 0.16130951788499626,
                  0.6531083254653984, 0.2532916025397821, 0.4663107728563063, 0.24442559200160274, 0.15896958364551972,
                  0.11037514116430513, 0.6563295894652734, 0.1381829513486138, 0.1965823616800535, 0.3687251706609641,
                  0.8209932298479351, 0.09710127579306127, 0.8379449074988039, 0.09609840789396307, 0.9764594650133958,
                  0.4686512016477016, 0.9767610881903371, 0.604845519745046, 0.7392635793983017, 0.039187792254320675,
                  0.2828069625764096, 0.1201965612131689, 0.29614019752214493, 0.11872771895424405, 0.317983179393976,
                  0.41426299451466997, 0.06414749634878436, 0.6924721193700198, 0.5666014542065752, 0.2653894909394454,
                  0.5232480534666997, 0.09394051075844168, 0.5759464955561793, 0.9292961975762141, 0.31856895245132366,
                  0.6674103799636817, 0.13179786240439217, 0.7163272041185655, 0.2894060929472011, 0.18319136200711683,
                  0.5865129348100832, 0.020107546187493552, 0.8289400292173631, 0.004695476192547066,
                  0.6778165367962301, 0.27000797319216485, 0.7351940221225949, 0.9621885451174382, 0.24875314351995803,
                  0.5761573344178369, 0.592041931271839, 0.5722519057908734, 0.2230816326406183, 0.952749011516985,
                  0.44712537861762736, 0.8464086724711278, 0.6994792753175043, 0.29743695085513366, 0.8137978197024772,
                  0.39650574084698464, 0.8811031971111616, 0.5812728726358587, 0.8817353618548528, 0.6925315900777659,
                  0.7252542798196405, 0.5013243819267023, 0.9560836347232239, 0.6439901992296374, 0.4238550485581797,
                  0.6063932141279244, 0.019193198309333526, 0.30157481667454933, 0.660173537492685, 0.29007760721044407,
                  0.6180154289988415, 0.42876870094576613, 0.13547406422245023, 0.29828232595603077, 0.5699649107012649,
                  0.5908727612481732, 0.5743252488495788, 0.6532008198571336, 0.6521032700016889, 0.43141843543397396,
                  0.896546595851063, 0.36756187004789653, 0.4358649252656268, 0.8919233550156721, 0.8061939890460857,
                  0.7038885835403663, 0.10022688731230112, 0.9194826137446735, 0.7142412995491114, 0.9988470065678665,
                  0.14944830465799375, 0.8681260573682142, 0.16249293467637482, 0.6155595642838442, 0.12381998284944151,
                  0.8480082293222344, 0.8073189587250107, 0.5691007386145933, 0.40718329722599966, 0.06916699545513805,
                  0.6974287731445636, 0.45354268267806885, 0.7220555994703479, 0.8663823259286292, 0.9755215050028858,
                  0.855803342392611, 0.011714084185001972, 0.3599780644783639, 0.729990562424058, 0.17162967726144052,
                  0.5210366062041293, 0.05433798833925363, 0.19999652489640007, 0.01852179446061397, 0.7936977033574206,
                  0.22392468806038013, 0.3453516806969027, 0.9280812934655909, 0.7044144019235328, 0.03183892953130785,
                  0.16469415649791275, 0.6214784014997635, 0.5772285886041676, 0.23789282137450862, 0.9342139979247938,
                  0.613965955965896, 0.5356328030249583, 0.589909976354571, 0.7301220295167696, 0.31194499547960186,
                  0.3982210622160919, 0.20984374897512215, 0.18619300588033616, 0.9443723899839336, 0.7395507950492876,
                  0.4904588086175671, 0.22741462797332324, 0.25435648177039294, 0.05802916032387562, 0.4344166255581208,
                  0.3117958819941026, 0.6963434888154595, 0.3777518392924809, 0.1796036775596348, 0.02467872839133123,
                  0.06724963146324858, 0.6793927734985673, 0.4536968445560453, 0.5365792111087222, 0.8966712930403421,
                  0.9903389473967044, 0.21689698439847394, 0.6630782031001008, 0.26332237673715064, 0.02065099946572868,
                  0.7583786538361414, 0.32001715082246784, 0.38346389417189797, 0.5883171135536057, 0.8310484552361904,
                  0.6289818435911487, 0.8726506554473953, 0.27354203481563577, 0.7980468339125637, 0.1856359443059522,
                  0.9527916569719446, 0.6874882763878153, 0.21550767711355845, 0.9473705904889242, 0.7308558067701578,
                  0.25394164259502583, 0.21331197736748198, 0.5182007139306632, 0.025662718054531575,
                  0.2074700754411094, 0.42468546875150626, 0.37416998033422555, 0.4635754243648107, 0.2776287062947319,
                  0.5867843464581688, 0.8638556059232314, 0.11753185596203308, 0.5173791071541142, 0.1320681063451533,
                  0.7168596811925937, 0.39605970280729375, 0.565421311858509, 0.18327983621407862, 0.14484775934337724,
                  0.48805628064895457, 0.3556127378499556, 0.940431945252813, 0.7653252538069653, 0.7486636198505473,
                  0.9037197397459334, 0.08342243544201855, 0.5521924699224066, 0.5844760689557689, 0.961936378547229,
                  0.29214752679254885, 0.24082877991544682, 0.10029394226549782, 0.016429629591474204,
                  0.9295293167921905, 0.66991654659091, 0.7851529120231378, 0.2817301057539491, 0.5864101661863267,
                  0.06395526612098112, 0.4856275959346229, 0.9774951397444468, 0.8765052453165908, 0.33815895183684563,
                  0.9615701545414985, 0.2317016264712045, 0.9493188224156814, 0.9413777047064986, 0.7992025873523917,
                  0.6304479368667911, 0.874287966624947, 0.2930202845077967, 0.8489435553129182, 0.6178766919175238,
                  0.01323685775889949, 0.34723351793221957, 0.14814086094816503, 0.9818293898182532,
                  0.47837030703998806, 0.4973913654986627, 0.6394725163987236, 0.3685846061296175, 0.13690027168559893,
                  0.8221177331942455, 0.18984791190275796, 0.511318982546456, 0.22431702897473926, 0.09784448449403405,
                  0.8621915174216833, 0.9729194890231303, 0.9608346580630002, 0.906555499221179, 0.7740473326986388,
                  0.3331451520286419, 0.08110138998799676, 0.40724117141380733, 0.23223414217094274,
                  0.13248763475798297, 0.05342718178682526, 0.7255943642105788, 0.011427458625031028,
                  0.7705807485027762, 0.14694664540037505, 0.07952208258675575, 0.08960303423860538, 0.6720478073539145,
                  0.24536720985284477, 0.42053946668009845, 0.5573687913239169, 0.8605511738287938, 0.7270442627113283,
                  0.27032790523871464, 0.1314827992911276, 0.05537432042119794, 0.3015986344809425, 0.26211814923967824,
                  0.45614056680047965, 0.6832813355476804, 0.6956254456388572, 0.28351884658216664, 0.3799269559001205,
                  0.18115096173690304, 0.7885455123065187, 0.0568480764332403, 0.6969972417249873, 0.7786953959411034,
                  0.7774075618487531, 0.25942256434535493, 0.37381313793256143, 0.587599635196389, 0.272821902424467,
                  0.3708527992178887, 0.19705428018563964, 0.4598558837560074, 0.044612301254114084, 0.799795884570618,
                  0.07695644698663273, 0.518835148831526, 0.3068100995451961, 0.5775429488313755, 0.9594333408334251,
                  0.6455702444560039, 0.03536243575549092, 0.43040243950806123, 0.5100168523182502, 0.536177494703452,
                  0.6813925106038379, 0.2775960977317661, 0.1288605654663202, 0.39267567654709434, 0.9564057227959488,
                  0.18713089175084474, 0.903983954928237, 0.5438059500773263, 0.4569114216457658, 0.8820414102298896,
                  0.45860396176858587, 0.7241676366115433, 0.399025321703102, 0.9040443929009577, 0.6900250201912274,
                  0.6996220542505167, 0.3277204015571189, 0.7567786427368892, 0.6360610554471413, 0.24002027337970955,
                  0.16053882248525642, 0.7963914745173317, 0.9591666030352225, 0.45813882726004285, 0.5909841653236849,
                  0.8577226441935546, 0.45722345335385706, 0.9518744768327362, 0.5757511620448724, 0.820767120701315,
                  0.9088437184127384, 0.8155238187685688, 0.15941446344895593, 0.6288984390617004, 0.3984342586196771,
                  0.0627129520233457, 0.42403225188984195, 0.2586840668894077, 0.8490383084285108, 0.03330462654669619,
                  0.9589827218634736, 0.3553688484719296, 0.3567068904025429, 0.01632850268370789, 0.18523232523618394,
                  0.4012595008036087, 0.9292914173027139, 0.09961493022127133, 0.9453015334790795, 0.8694885305466322,
                  0.4541623969075518, 0.32670088176826007, 0.23274412927905685, 0.6144647064768743, 0.03307459147550562,
                  0.015606064446828216, 0.42879572249823783, 0.0680740739747202, 0.2519409882460929,
                  0.22116091534608384, 0.2531911937228519, 0.13105523121525775, 0.01203622289765427,
                  0.11548429713874808, 0.6184802595127479, 0.9742562128180503, 0.9903450015608939, 0.4090540953730616,
                  0.16295442604660537, 0.6387617573665293, 0.49030534654873714, 0.9894097772844315, 0.0653042071517802,
                  0.7832344383138131, 0.2883984973314939, 0.241418620076574, 0.662504571532676, 0.24606318499096447,
                  0.6658591175591877, 0.5173085172022888, 0.4240889884358493, 0.554687808661419, 0.28705151991962974,
                  0.7065747062729789, 0.414856869333564, 0.36054556048589226, 0.8286569145557378, 0.9249669119531921,
                  0.046007310887296926, 0.2326269928297655, 0.34851936949256324, 0.8149664793702474, 0.9854914276432976,
                  0.9689717046703518, 0.9049483455499269, 0.2965562650640299, 0.9920112434144741, 0.2494200410564512,
                  0.1059061548822322, 0.9509526110553941, 0.2334202554680963, 0.6897682650777505, 0.05835635898058866,
                  0.7307090991274762, 0.8817202123338397, 0.2724368954659625, 0.37905689607742854, 0.3742961833209161,
                  0.7487882575401331, 0.23780724253903884, 0.171853099047643, 0.4492916486877381, 0.3044684073773195,
                  0.8391891222586524, 0.23774182601563876, 0.5023894574892614, 0.9425835996979304, 0.6339976977446607,
                  0.8672894054624648, 0.9402096893547673, 0.7507648618863519, 0.6995750602247514, 0.9679655666042271,
                  0.9944007896476794, 0.45182168266975964, 0.07086977818420837, 0.29279403144051885,
                  0.15235470568773046, 0.4174863747960118, 0.13128932847325603, 0.604117804020882, 0.3828080591578541,
                  0.89538588428821, 0.9677946717985019, 0.5468849016694222, 0.2748235698675966, 0.5922304187618368,
                  0.8967611582244098, 0.4067333458357483, 0.5520782766919708, 0.2716527676061459, 0.455444149450027,
                  0.40171353537959864, 0.24841346508297102, 0.5058663838253084, 0.3103808259798114, 0.37303486388074747,
                  0.5249704422542643, 0.7505950229289875, 0.3335074657912753, 0.9241587666207636, 0.8623185468359024,
                  0.04869029597552854, 0.25364252425682277, 0.4461355126592019, 0.10462788874247408, 0.3484759890334971,
                  0.7400975256176825, 0.6805144811428259, 0.6223844285660048, 0.7105284027223456, 0.20492368695970176,
                  0.3416981148647321, 0.6762424822774629, 0.8792347630313271, 0.5436780538280952, 0.2826996509455366,
                  0.03023525800598259, 0.7103368289742134, 0.007884103508440377, 0.3726790698209955, 0.5305372145627818,
                  0.922111461767193, 0.08949454503290011, 0.4059423219682837, 0.024313199710157773, 0.3426109843415903,
                  0.6222310588397949, 0.2790679482285984, 0.2097499496556351, 0.11570323332709365, 0.5771402440203418,
                  0.695270005904686, 0.6719571405958223, 0.9488610207205074, 0.0027032138935026984, 0.647196653894036,
                  0.6003922370976396, 0.5887396099702882, 0.9627703198402424, 0.016871673370039697, 0.6964824307014501,
                  0.8136786497018634, 0.5098071966215841, 0.33396486959680916, 0.7908401632274049, 0.09724292563242465,
                  0.44203563772992527, 0.5199523745708382, 0.6939564109345475, 0.09088573203240946, 0.22775950153786095,
                  0.41030156269012563, 0.6232946730201306, 0.8869607812174175, 0.6188261682413765, 0.13346147093493443,
                  0.9805801327872824, 0.8717857347554929, 0.502720761145324, 0.922347981796633, 0.5413807937571358,
                  0.9233060678891631, 0.8298973686033432, 0.9682864102942973, 0.9197828107781584, 0.03603381742856948,
                  0.17477200416093996, 0.38913467710118577, 0.9521426972954208, 0.3000289194759296, 0.16046764388760104,
                  0.8863046660865599, 0.4463944154832029, 0.9078755943543261, 0.16023046632014326, 0.6611175115080995,
                  0.4402637528294918, 0.0764867690302854, 0.6964631446525006, 0.2473987555391537, 0.039615522579517726,
                  0.05994429824957326, 0.06107853706678734, 0.9077329574850395, 0.739883917829101, 0.8980623572137351,
                  0.6725823112965214, 0.5289399290308832, 0.30444636434737826, 0.9979622513286734, 0.36218905893938935,
                  0.47064894921390954, 0.37824517492346177, 0.9795269293354586, 0.17465838539500578,
                  0.32798800090807967, 0.6803486660150015, 0.06320761833863064, 0.607249374011541, 0.4776465028764161,
                  0.2839999767621011, 0.238413280924058, 0.5145127432987567, 0.36792758053704133, 0.45651989126265535,
                  0.3374773817642399, 0.9704936935959776, 0.13343943174560402, 0.0968039531783742, 0.34339172879091606,
                  0.5910269008704913, 0.6591764718500283, 0.39725674716804205, 0.9992779939221711, 0.3518929961930426,
                  0.7214066679599525, 0.6375826945307929, 0.8130538632474607, 0.97622566345382, 0.8897936564455402,
                  0.7645619743577086, 0.6982484778182906, 0.3354981696758996, 0.14768557820670736, 0.06263600305980976,
                  0.24190170420148482, 0.4322814811812986, 0.5219962736299825, 0.7730835540548716, 0.958740923056593,
                  0.11732048038481102, 0.10700414019379156, 0.5896947230135507, 0.745398073947293, 0.8481503803469849,
                  0.9358320802167885, 0.983426242260642, 0.3998016922245259, 0.3803351835275731, 0.14780867669727238,
                  0.6849344386835594, 0.6567619584408371, 0.8620625958512073, 0.09725799478764063, 0.4977769078253418,
                  0.5810819296720631, 0.2415570400399184, 0.16902540612916128, 0.8595808364196215, 0.058534922235558784,
                  0.4706209039180729, 0.11583400130088528, 0.45705876133136736, 0.9799623263423093, 0.4237063534554728,
                  0.8571249175045673, 0.11731556418319389, 0.27125207676186414, 0.4037927406673345, 0.39981214000933074,
                  0.6713834786701531, 0.34471812737550767, 0.7137668684100164, 0.6391868992253925, 0.3991611452547731,
                  0.43176012765431926, 0.6145276998103207, 0.07004219014464463, 0.8224067383556903, 0.6534211611136369,
                  0.7263424644178352, 0.5369230010823904, 0.11047711099174473, 0.4050356132969499, 0.40537358284855607,
                  0.3210429900432169, 0.02995032490474936, 0.7372542425964773, 0.1097844580625007, 0.6063081330450851,
                  0.7032174964672158, 0.6347863229336947, 0.959142251977975, 0.10329815508513862, 0.8671671591051991,
                  0.029190234848913255, 0.534916854927084, 0.4042436179392588, 0.5241838603937582, 0.3650998770600098,
                  0.19056691494006806, 0.01912289744868978, 0.5181498137911743, 0.8427768626848423, 0.37321595574479416,
                  0.22286381801498023, 0.08053200347184408, 0.08531092311870336, 0.22139644629277222,
                  0.10001406092155518, 0.26503969836448205, 0.06614946211695472, 0.06560486720992564,
                  0.8562761796227819, 0.16212026070883323, 0.559682405823448, 0.7734555444490305, 0.4564095653390666,
                  0.15336887785934572, 0.19959614212011434, 0.43298420628118073, 0.5282340891785358, 0.3494402920485349,
                  0.7814796002346613, 0.7510216488563984, 0.9272118073731179, 0.02895254902696054, 0.8956912912102034,
                  0.39256878846215104, 0.8783724953799941, 0.6907847761565296, 0.9873487570739682, 0.7592824517166681,
                  0.3645446259967866, 0.5010631728347521, 0.37638915519435123, 0.3649118360212381, 0.26090449938105975,
                  0.4959702953734696, 0.6817399450693612, 0.2773402713052435, 0.52437981107722, 0.11738029417055718,
                  0.1598452868541913, 0.04680635471218875, 0.9707314427706328, 0.0038603515102610952,
                  0.1785799680576563, 0.6128667531169923, 0.0813695988533053, 0.8818965030968323, 0.7196201578422882,
                  0.9663899714378934, 0.5076355472407649, 0.3004036831584872, 0.5495005727952714, 0.9308187172979732,
                  0.5207614372418605, 0.26720703186231864, 0.8773987891741196, 0.3719187485124613,
                  0.0013833499989991394, 0.24768502249231594, 0.3182335091770624, 0.8587774682319022, 0.458503167066445,
                  0.44458728781130075, 0.3361022663998874, 0.8806781230470796, 0.9450267769403914, 0.9918903291546294,
                  0.37674126696098675, 0.9661474456271714, 0.7918795696309013, 0.6756891476442668, 0.24488947942009942,
                  0.2164572609442098, 0.16604782452124567, 0.9227566102253654, 0.2940766623831661, 0.45309424544887844,
                  0.4939578339872235, 0.7781715954502542, 0.8442349615530241, 0.1390727011486128, 0.4269043602110737,
                  0.8428548878354571, 0.8180333057558384, 0.1024137584524164])
    b = np.array([0.4236547993389047, 0.6458941130666561, 0.4375872112626925, 0.8917730007820798, 0.9636627605010293,
                  0.3834415188257777, 0.7917250380826646, 0.5288949197529045, 0.5680445610939323, 0.925596638292661,
                  0.07103605819788694, 0.08712929970154071, 0.02021839744032572, 0.832619845547938, 0.7781567509498505,
                  0.8700121482468192, 0.978618342232764, 0.7991585642167236, 0.46147936225293185, 0.7805291762864555,
                  0.11827442586893322, 0.6399210213275238, 0.1433532874090464, 0.9446689170495839, 0.5218483217500717,
                  0.0, 0.0, 0.0, 0.45615033221654855, 0.5684339488686485, 0.018789800436355142, 0.6176354970758771,
                  0.6120957227224214, 0.6169339968747569, 0.9437480785146242, 0.6818202991034834, 0.359507900573786,
                  0.43703195379934145, 0.6976311959272649, 0.06022547162926983, 0.6667667154456677, 0.6706378696181594,
                  0.2103825610738409, 0.1289262976548533, 0.31542835092418386, 0.3637107709426226, 0.5701967704178796,
                  0.43860151346232035, 0.9883738380592262, 0.10204481074802807, 0.2088767560948347, 0.16130951788499626,
                  0.6531083254653984, 0.0, 0.0, 0.0, 0.15896958364551972, 0.11037514116430513, 0.6563295894652734,
                  0.1381829513486138, 0.1965823616800535, 0.3687251706609641, 0.8209932298479351, 0.09710127579306127,
                  0.8379449074988039, 0.09609840789396307, 0.9764594650133958, 0.4686512016477016, 0.9767610881903371,
                  0.604845519745046, 0.7392635793983017, 0.039187792254320675, 0.2828069625764096, 0.1201965612131689,
                  0.29614019752214493, 0.11872771895424405, 0.317983179393976, 0.41426299451466997, 0.06414749634878436,
                  0.6924721193700198, 0.5666014542065752, 0.0, 0.0, 0.0, 0.5759464955561793, 0.9292961975762141,
                  0.31856895245132366, 0.6674103799636817, 0.13179786240439217, 0.7163272041185655, 0.2894060929472011,
                  0.18319136200711683, 0.5865129348100832, 0.020107546187493552, 0.8289400292173631,
                  0.004695476192547066, 0.6778165367962301, 0.27000797319216485, 0.7351940221225949, 0.9621885451174382,
                  0.24875314351995803, 0.5761573344178369, 0.592041931271839, 0.5722519057908734, 0.2230816326406183,
                  0.952749011516985, 0.44712537861762736, 0.8464086724711278, 0.6994792753175043, 0.0, 0.0, 0.0,
                  0.8811031971111616, 0.5812728726358587, 0.8817353618548528, 0.6925315900777659, 0.7252542798196405,
                  0.5013243819267023, 0.9560836347232239, 0.6439901992296374, 0.4238550485581797, 0.6063932141279244,
                  0.019193198309333526, 0.30157481667454933, 0.660173537492685, 0.29007760721044407, 0.6180154289988415,
                  0.42876870094576613, 0.13547406422245023, 0.29828232595603077, 0.5699649107012649, 0.5908727612481732,
                  0.5743252488495788, 0.6532008198571336, 0.6521032700016889, 0.43141843543397396, 0.896546595851063,
                  0.0, 0.0, 0.0, 0.8061939890460857, 0.7038885835403663, 0.10022688731230112, 0.9194826137446735,
                  0.7142412995491114, 0.9988470065678665, 0.14944830465799375, 0.8681260573682142, 0.16249293467637482,
                  0.6155595642838442, 0.12381998284944151, 0.8480082293222344, 0.8073189587250107, 0.5691007386145933,
                  0.40718329722599966, 0.06916699545513805, 0.6974287731445636, 0.45354268267806885, 0.7220555994703479,
                  0.8663823259286292, 0.9755215050028858, 0.855803342392611, 0.011714084185001972, 0.3599780644783639,
                  0.729990562424058, 0.0, 0.0, 0.0, 0.19999652489640007, 0.01852179446061397, 0.7936977033574206,
                  0.22392468806038013, 0.3453516806969027, 0.9280812934655909, 0.7044144019235328, 0.03183892953130785,
                  0.16469415649791275, 0.6214784014997635, 0.5772285886041676, 0.23789282137450862, 0.9342139979247938,
                  0.613965955965896, 0.5356328030249583, 0.589909976354571, 0.7301220295167696, 0.31194499547960186,
                  0.3982210622160919, 0.20984374897512215, 0.18619300588033616, 0.9443723899839336, 0.7395507950492876,
                  0.4904588086175671, 0.22741462797332324, 0.0, 0.0, 0.0, 0.3117958819941026, 0.6963434888154595,
                  0.3777518392924809, 0.1796036775596348, 0.02467872839133123, 0.06724963146324858, 0.6793927734985673,
                  0.4536968445560453, 0.5365792111087222, 0.8966712930403421, 0.9903389473967044, 0.21689698439847394,
                  0.6630782031001008, 0.26332237673715064, 0.02065099946572868, 0.7583786538361414, 0.32001715082246784,
                  0.38346389417189797, 0.5883171135536057, 0.8310484552361904, 0.6289818435911487, 0.8726506554473953,
                  0.27354203481563577, 0.7980468339125637, 0.1856359443059522, 0.0, 0.0, 0.0, 0.9473705904889242,
                  0.7308558067701578, 0.25394164259502583, 0.21331197736748198, 0.5182007139306632,
                  0.025662718054531575, 0.2074700754411094, 0.42468546875150626, 0.37416998033422555,
                  0.4635754243648107, 0.2776287062947319, 0.5867843464581688, 0.8638556059232314, 0.11753185596203308,
                  0.5173791071541142, 0.1320681063451533, 0.7168596811925937, 0.39605970280729375, 0.565421311858509,
                  0.18327983621407862, 0.14484775934337724, 0.48805628064895457, 0.3556127378499556, 0.940431945252813,
                  0.7653252538069653, 0.0, 0.0, 0.0, 0.5521924699224066, 0.5844760689557689, 0.961936378547229,
                  0.29214752679254885, 0.24082877991544682, 0.10029394226549782, 0.016429629591474204,
                  0.9295293167921905, 0.66991654659091, 0.7851529120231378, 0.2817301057539491, 0.5864101661863267,
                  0.06395526612098112, 0.4856275959346229, 0.9774951397444468, 0.8765052453165908, 0.33815895183684563,
                  0.9615701545414985, 0.2317016264712045, 0.9493188224156814, 0.9413777047064986, 0.7992025873523917,
                  0.6304479368667911, 0.874287966624947, 0.2930202845077967, 0.0, 0.0, 0.0, 0.34723351793221957,
                  0.14814086094816503, 0.9818293898182532, 0.47837030703998806, 0.4973913654986627, 0.6394725163987236,
                  0.3685846061296175, 0.13690027168559893, 0.8221177331942455, 0.18984791190275796, 0.511318982546456,
                  0.22431702897473926, 0.09784448449403405, 0.8621915174216833, 0.9729194890231303, 0.9608346580630002,
                  0.906555499221179, 0.7740473326986388, 0.3331451520286419, 0.08110138998799676, 0.40724117141380733,
                  0.23223414217094274, 0.13248763475798297, 0.05342718178682526, 0.7255943642105788, 0.0, 0.0, 0.0,
                  0.07952208258675575, 0.08960303423860538, 0.6720478073539145, 0.24536720985284477,
                  0.42053946668009845, 0.5573687913239169, 0.8605511738287938, 0.7270442627113283, 0.27032790523871464,
                  0.1314827992911276, 0.05537432042119794, 0.3015986344809425, 0.26211814923967824, 0.45614056680047965,
                  0.6832813355476804, 0.6956254456388572, 0.28351884658216664, 0.3799269559001205, 0.18115096173690304,
                  0.7885455123065187, 0.0568480764332403, 0.6969972417249873, 0.7786953959411034, 0.7774075618487531,
                  0.25942256434535493, 0.0, 0.0, 0.0, 0.3708527992178887, 0.19705428018563964, 0.4598558837560074,
                  0.044612301254114084, 0.799795884570618, 0.07695644698663273, 0.518835148831526, 0.3068100995451961,
                  0.5775429488313755, 0.9594333408334251, 0.6455702444560039, 0.03536243575549092, 0.43040243950806123,
                  0.5100168523182502, 0.536177494703452, 0.6813925106038379, 0.2775960977317661, 0.1288605654663202,
                  0.39267567654709434, 0.9564057227959488, 0.18713089175084474, 0.903983954928237, 0.5438059500773263,
                  0.4569114216457658, 0.8820414102298896, 0.0, 0.0, 0.0, 0.9040443929009577, 0.6900250201912274,
                  0.6996220542505167, 0.3277204015571189, 0.7567786427368892, 0.6360610554471413, 0.24002027337970955,
                  0.16053882248525642, 0.7963914745173317, 0.9591666030352225, 0.45813882726004285, 0.5909841653236849,
                  0.8577226441935546, 0.45722345335385706, 0.9518744768327362, 0.5757511620448724, 0.820767120701315,
                  0.9088437184127384, 0.8155238187685688, 0.15941446344895593, 0.6288984390617004, 0.3984342586196771,
                  0.0627129520233457, 0.42403225188984195, 0.2586840668894077, 0.0, 0.0, 0.0, 0.3553688484719296,
                  0.3567068904025429, 0.01632850268370789, 0.18523232523618394, 0.4012595008036087, 0.9292914173027139,
                  0.09961493022127133, 0.9453015334790795, 0.8694885305466322, 0.4541623969075518, 0.32670088176826007,
                  0.23274412927905685, 0.6144647064768743, 0.03307459147550562, 0.015606064446828216,
                  0.42879572249823783, 0.0680740739747202, 0.2519409882460929, 0.22116091534608384, 0.2531911937228519,
                  0.13105523121525775, 0.01203622289765427, 0.11548429713874808, 0.6184802595127479, 0.9742562128180503,
                  0.0, 0.0, 0.0, 0.6387617573665293, 0.49030534654873714, 0.9894097772844315, 0.0653042071517802,
                  0.7832344383138131, 0.2883984973314939, 0.241418620076574, 0.662504571532676, 0.24606318499096447,
                  0.6658591175591877, 0.5173085172022888, 0.4240889884358493, 0.554687808661419, 0.28705151991962974,
                  0.7065747062729789, 0.414856869333564, 0.36054556048589226, 0.8286569145557378, 0.9249669119531921,
                  0.046007310887296926, 0.2326269928297655, 0.34851936949256324, 0.8149664793702474, 0.9854914276432976,
                  0.9689717046703518, 0.0, 0.0, 0.0, 0.2494200410564512, 0.1059061548822322, 0.9509526110553941,
                  0.2334202554680963, 0.6897682650777505, 0.05835635898058866, 0.7307090991274762, 0.8817202123338397,
                  0.2724368954659625, 0.37905689607742854, 0.3742961833209161, 0.7487882575401331, 0.23780724253903884,
                  0.171853099047643, 0.4492916486877381, 0.3044684073773195, 0.8391891222586524, 0.23774182601563876,
                  0.5023894574892614, 0.9425835996979304, 0.6339976977446607, 0.8672894054624648, 0.9402096893547673,
                  0.7507648618863519, 0.6995750602247514, 0.0, 0.0, 0.0, 0.07086977818420837, 0.29279403144051885,
                  0.15235470568773046, 0.4174863747960118, 0.13128932847325603, 0.604117804020882, 0.3828080591578541,
                  0.89538588428821, 0.9677946717985019, 0.5468849016694222, 0.2748235698675966, 0.5922304187618368,
                  0.8967611582244098, 0.4067333458357483, 0.5520782766919708, 0.2716527676061459, 0.455444149450027,
                  0.40171353537959864, 0.24841346508297102, 0.5058663838253084, 0.3103808259798114, 0.37303486388074747,
                  0.5249704422542643, 0.7505950229289875, 0.3335074657912753, 0.0, 0.0, 0.0, 0.25364252425682277,
                  0.4461355126592019, 0.10462788874247408, 0.3484759890334971, 0.7400975256176825, 0.6805144811428259,
                  0.6223844285660048, 0.7105284027223456, 0.20492368695970176, 0.3416981148647321, 0.6762424822774629,
                  0.8792347630313271, 0.5436780538280952, 0.2826996509455366, 0.03023525800598259, 0.7103368289742134,
                  0.007884103508440377, 0.3726790698209955, 0.5305372145627818, 0.922111461767193, 0.08949454503290011,
                  0.4059423219682837, 0.024313199710157773, 0.3426109843415903, 0.6222310588397949, 0.0, 0.0, 0.0,
                  0.5771402440203418, 0.695270005904686, 0.6719571405958223, 0.9488610207205074, 0.0027032138935026984,
                  0.647196653894036, 0.6003922370976396, 0.5887396099702882, 0.9627703198402424, 0.016871673370039697,
                  0.6964824307014501, 0.8136786497018634, 0.5098071966215841, 0.33396486959680916, 0.7908401632274049,
                  0.09724292563242465, 0.44203563772992527, 0.5199523745708382, 0.6939564109345475, 0.09088573203240946,
                  0.22775950153786095, 0.41030156269012563, 0.6232946730201306, 0.8869607812174175, 0.6188261682413765,
                  0.0, 0.0, 0.0, 0.502720761145324, 0.922347981796633, 0.5413807937571358, 0.9233060678891631,
                  0.8298973686033432, 0.9682864102942973, 0.9197828107781584, 0.03603381742856948, 0.17477200416093996,
                  0.38913467710118577, 0.9521426972954208, 0.3000289194759296, 0.16046764388760104, 0.8863046660865599,
                  0.4463944154832029, 0.9078755943543261, 0.16023046632014326, 0.6611175115080995, 0.4402637528294918,
                  0.0764867690302854, 0.6964631446525006, 0.2473987555391537, 0.039615522579517726, 0.05994429824957326,
                  0.06107853706678734, 0.0, 0.0, 0.0, 0.6725823112965214, 0.5289399290308832, 0.30444636434737826,
                  0.9979622513286734, 0.36218905893938935, 0.47064894921390954, 0.37824517492346177, 0.9795269293354586,
                  0.17465838539500578, 0.32798800090807967, 0.6803486660150015, 0.06320761833863064, 0.607249374011541,
                  0.4776465028764161, 0.2839999767621011, 0.238413280924058, 0.5145127432987567, 0.36792758053704133,
                  0.45651989126265535, 0.3374773817642399, 0.9704936935959776, 0.13343943174560402, 0.0968039531783742,
                  0.34339172879091606, 0.5910269008704913, 0.0, 0.0, 0.0, 0.3518929961930426, 0.7214066679599525,
                  0.6375826945307929, 0.8130538632474607, 0.97622566345382, 0.8897936564455402, 0.7645619743577086,
                  0.6982484778182906, 0.3354981696758996, 0.14768557820670736, 0.06263600305980976, 0.24190170420148482,
                  0.4322814811812986, 0.5219962736299825, 0.7730835540548716, 0.958740923056593, 0.11732048038481102,
                  0.10700414019379156, 0.5896947230135507, 0.745398073947293, 0.8481503803469849, 0.9358320802167885,
                  0.983426242260642, 0.3998016922245259, 0.3803351835275731, 0.0, 0.0, 0.0, 0.8620625958512073,
                  0.09725799478764063, 0.4977769078253418, 0.5810819296720631, 0.2415570400399184, 0.16902540612916128,
                  0.8595808364196215, 0.058534922235558784, 0.4706209039180729, 0.11583400130088528,
                  0.45705876133136736, 0.9799623263423093, 0.4237063534554728, 0.8571249175045673, 0.11731556418319389,
                  0.27125207676186414, 0.4037927406673345, 0.39981214000933074, 0.6713834786701531, 0.34471812737550767,
                  0.7137668684100164, 0.6391868992253925, 0.3991611452547731, 0.43176012765431926, 0.6145276998103207,
                  0.0, 0.0, 0.0, 0.7263424644178352, 0.5369230010823904, 0.11047711099174473, 0.4050356132969499,
                  0.40537358284855607, 0.3210429900432169, 0.02995032490474936, 0.7372542425964773, 0.1097844580625007,
                  0.6063081330450851, 0.7032174964672158, 0.6347863229336947, 0.959142251977975, 0.10329815508513862,
                  0.8671671591051991, 0.029190234848913255, 0.534916854927084, 0.4042436179392588, 0.5241838603937582,
                  0.3650998770600098, 0.19056691494006806, 0.01912289744868978, 0.5181498137911743, 0.8427768626848423,
                  0.37321595574479416, 0.0, 0.0, 0.0, 0.22139644629277222, 0.10001406092155518, 0.26503969836448205,
                  0.06614946211695472, 0.06560486720992564, 0.8562761796227819, 0.16212026070883323, 0.559682405823448,
                  0.7734555444490305, 0.4564095653390666, 0.15336887785934572, 0.19959614212011434, 0.43298420628118073,
                  0.5282340891785358, 0.3494402920485349, 0.7814796002346613, 0.7510216488563984, 0.9272118073731179,
                  0.02895254902696054, 0.8956912912102034, 0.39256878846215104, 0.8783724953799941, 0.6907847761565296,
                  0.9873487570739682, 0.7592824517166681, 0.0, 0.0, 0.0, 0.3649118360212381, 0.26090449938105975,
                  0.4959702953734696, 0.6817399450693612, 0.2773402713052435, 0.52437981107722, 0.11738029417055718,
                  0.1598452868541913, 0.04680635471218875, 0.9707314427706328, 0.0038603515102610952,
                  0.1785799680576563, 0.6128667531169923, 0.0813695988533053, 0.8818965030968323, 0.7196201578422882,
                  0.9663899714378934, 0.5076355472407649, 0.3004036831584872, 0.5495005727952714, 0.9308187172979732,
                  0.5207614372418605, 0.26720703186231864, 0.8773987891741196, 0.3719187485124613, 0.0, 0.0, 0.0,
                  0.8587774682319022, 0.458503167066445, 0.44458728781130075, 0.3361022663998874, 0.8806781230470796,
                  0.9450267769403914, 0.9918903291546294, 0.37674126696098675, 0.9661474456271714, 0.7918795696309013,
                  0.6756891476442668, 0.24488947942009942, 0.2164572609442098, 0.16604782452124567, 0.9227566102253654,
                  0.2940766623831661, 0.45309424544887844, 0.4939578339872235, 0.7781715954502542, 0.8442349615530241,
                  0.1390727011486128, 0.4269043602110737, 0.8428548878354571, 0.8180333057558384, 0.1024137584524164,
                  0.0, 0.0, 0.0])
    np.random.seed(0)
    np.testing.assert_allclose(tform(a), b)


def submit_random_crop():
    np.random.seed(0)
    for _ in range(20):
        tform = ndl.data.RandomCrop(np.random.randint(1, 2))
        size = np.random.randint(0, 4)
        mugrade.submit(tform(np.random.rand(size, size).flatten()))

def test_sequential_sampler():
    for a in [np.random.rand(1000, 5), np.arange(50)]:
        a = np.random.rand(1000, 5)
        sampler = ndl.data.SequentialSampler(a)
        assert len(sampler) == len(a)
        for i, sampled_idx in enumerate(sampler):
            assert i == sampled_idx, f'{i} does not equal {sampled_idx}'

def submit_sequential_sampler():
    for a in [np.random.rand(23, 2), np.arange(25)]:
        sampler = ndl.data.SequentialSampler(a)
        idxs = [i for i in sampler]
        mugrade.submit(idxs)


def test_random_sampler():
    a = np.random.rand(100)
    sampler = ndl.data.RandomSampler(a, False)
    assert len(sampler) == 100
    have_seen = {}
    for i in sampler:
        if i in have_seen:
            assert False, f'The sampler should sample without replacement, but we saw the same index:L {i}'
        have_seen[i] = 1
    del have_seen

    a = np.random.rand(100)
    sampler = ndl.data.RandomSampler(a, True)
    assert len(sampler) == 100
    have_seen = {}
    for i in sampler:
        if i in have_seen:
            have_seen[i] += 1
        else:
            have_seen[i] = 1
    multiple = False
    for val in have_seen.values():
        if val > 1:
            multiple = True
    assert multiple, 'you are not sampling with replacement'
    del have_seen

    a = np.random.rand(100)
    sampler = ndl.data.RandomSampler(a, True, 1000)
    assert len(sampler) == 1000

    a = np.random.rand(100)
    sampler = ndl.data.RandomSampler(a, True, 10)
    assert len(sampler) == 10

def submit_random_sampler():
    a = np.random.rand(28)
    sampler = ndl.data.RandomSampler(a, False)
    mugrade.submit(sum([x for x in sampler]))


def test_batch_sampler():
    a = np.random.rand(100)
    sampler = ndl.data.SequentialSampler(a)
    batch_sampler = ndl.data.BatchSampler(sampler, 10, True)
    assert len(batch_sampler) == 10, 'the length of the batch sampler is incorrect'
    idxs = []
    for x in batch_sampler:
        assert len(x) == 10, 'the batches being returned are the wrong size.'
        idxs.extend(x)
    assert sum(idxs) == sum([x for x in range(100)]), 'the returned indexes are not correct'

    a = np.random.rand(104)
    sampler = ndl.data.SequentialSampler(a)
    batch_sampler = ndl.data.BatchSampler(sampler, 10, True)
    assert len(batch_sampler) == 10, 'the length of the batch sampler is incorrect'
    for x in batch_sampler:
        assert len(x) == 10, 'the batches being returned are the wrong size.'

    a = np.random.rand(104)
    sampler = ndl.data.SequentialSampler(a)
    batch_sampler = ndl.data.BatchSampler(sampler, 10, False)
    assert len(batch_sampler) == 11, 'the length of the batch sampler is incorrect'
    idxs = []
    for x in batch_sampler:
        assert len(x) == 10 or len(x) == 4, 'the batches being returned are the wrong size.'
        idxs.extend(x)
    assert sum(idxs) == sum([x for x in range(104)]), 'the returned indexes are not correct'


def submit_batch_sampler():
    a = np.random.rand(23)
    sampler = ndl.data.SequentialSampler(a)
    batch_sampler = ndl.data.BatchSampler(sampler, 15, False)
    mugrade.submit([x for x in batch_sampler])

    sampler = ndl.data.SequentialSampler(a)
    batch_sampler = ndl.data.BatchSampler(sampler, 11, True)
    mugrade.submit([x for x in batch_sampler])


def test_collate_mnist():
    fake_x = np.random.rand(100, 25)
    fake_y = np.random.randint(0, 10, 100)
    data = [(x, y) for x, y in zip(fake_x, fake_y)]

    x, y = ndl.data.collate_mnist(data)
    assert type(x) == type(ndl.Tensor([1]))

    np.testing.assert_allclose(fake_x, x.numpy())
    np.testing.assert_allclose(fake_y, y.numpy())

    fake_x = np.random.rand(10)
    fake_y = 1
    x, y = ndl.data.collate_mnist((fake_x, fake_y))
    np.testing.assert_allclose(fake_x, x.numpy())
    np.testing.assert_allclose(fake_y, y.numpy())

    # add a test for when the mnist dataset is ready
    X, y = ndl.data.parse_mnist("data/train-images-idx3-ubyte.gz",
                                "data/train-labels-idx1-ubyte.gz")
    a, b = ndl.data.collate_mnist((X[0], y[0]))
    np.testing.assert_allclose(a.numpy(), X[0])
    np.testing.assert_allclose(b.numpy(), y[0])

    data = [(a, b) for a, b in zip(X[:50], y[:50])]
    a, b = ndl.data.collate_mnist(data)
    np.testing.assert_allclose(a.numpy(), X[:50])
    np.testing.assert_allclose(b.numpy(), y[:50])


def submit_collate_mnist():
    np.random.seed(0)
    for i in [3, 5, 8]:
        fake_x = np.random.rand(i, 7)
        fake_y = np.random.randint(0, 10, i)
        data = [(x, y) for x, y in zip(fake_x, fake_y)]

        x, y = ndl.data.collate_mnist(data)
        mugrade.submit(x.numpy())
        mugrade.submit(y.numpy())


def test_mnist_dataset():
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz")
    X, y = ndl.data.parse_mnist("data/train-images-idx3-ubyte.gz",
                                "data/train-labels-idx1-ubyte.gz")
    assert len(mnist_train_dataset) == len(X)

    for i in np.random.randint(0, len(X), 100):
        np.testing.assert_allclose(mnist_train_dataset[i][0], X[i])
        np.testing.assert_allclose(mnist_train_dataset[i][1], y[i])

    mnist_train_dataset = ndl.data.MNISTDataset("data/t10k-images-idx3-ubyte.gz",
                                               "data/t10k-labels-idx1-ubyte.gz")
    X, y = ndl.data.parse_mnist("data/t10k-images-idx3-ubyte.gz",
                                "data/t10k-labels-idx1-ubyte.gz")
    assert len(mnist_train_dataset) == len(X)

    for i in np.random.randint(0, len(X), 100):
        np.testing.assert_allclose(mnist_train_dataset[i][0], X[i])
        np.testing.assert_allclose(mnist_train_dataset[i][1], y[i])

    # test a transform
    tforms = [ndl.data.RandomCrop(3), ndl.data.FlipHorizontal()]
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz",
                                                p=1,
                                                transforms=tforms)
    X, y = ndl.data.parse_mnist("data/train-images-idx3-ubyte.gz",
                                "data/train-labels-idx1-ubyte.gz")
    modified = True
    effect = False
    for i in range(len(mnist_train_dataset)):
        x_mod, y_served = mnist_train_dataset[i]
        if np.allclose(x_mod, X[i]):
            modified = False
        elif not np.allclose(x_mod, X[i]) and not effect:
            effect = True

    assert effect, 'there seems to be no effect on the data being served'
    assert modified, 'the transform was not applied to all datapoints, when p=1.'

    # test a transform
    tforms = [ndl.data.RandomCrop(3), ndl.data.FlipHorizontal()]
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz",
                                                p=0.5,
                                                transforms=tforms)
    X, y = ndl.data.parse_mnist("data/train-images-idx3-ubyte.gz",
                                "data/train-labels-idx1-ubyte.gz")
    same_sample = False
    diff_sample = False
    for i in range(len(mnist_train_dataset)):
        x_mod, y_served = mnist_train_dataset[i]
        if np.allclose(x_mod, X[i]):
            same_sample = True

        if not np.allclose(x_mod, X[i]):
            diff_sample = True

    assert same_sample, 'p was 0.5, but no samples remained the same'
    assert diff_sample, 'p was 0.5, but no samples were modified'



def submit_mnist_dataset():
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz")
    mugrade.submit(mnist_train_dataset[69][:25])
    mugrade.submit(len(mnist_train_dataset))

    tforms = [ndl.data.FlipHorizontal()]
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz",
                                                p=1,
                                                transforms=tforms)
    for i in [822, 69, 420, 96]:
        mugrade.submit(mnist_train_dataset[i][:-25])




def test_dataloader():
    batch_size = 1
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz")
    mnist_train_dataloader = ndl.data.DataLoader(dataset=mnist_train_dataset,
                                                 batch_size=batch_size,
                                                 shuffle=False,
                                                 collate_fn=ndl.data.collate_mnist,
                                                 drop_last=False)

    for i, batch in enumerate(mnist_train_dataloader):
        batch_x, batch_y = batch[0].numpy(), batch[1].numpy()
        truth = mnist_train_dataset[i * batch_size:(i + 1) * batch_size]
        truth_x = truth[0] if truth[0].shape[0] > 1 else truth[0].reshape(-1)
        truth_y = truth[1] if truth[1].shape[0] > 1 else truth[1].reshape(-1)

        np.testing.assert_allclose(truth_x, batch_x)
        np.testing.assert_allclose(batch_y, truth_y)

    batch_size = 5
    mnist_test_dataset = ndl.data.MNISTDataset("data/t10k-images-idx3-ubyte.gz",
                                               "data/t10k-labels-idx1-ubyte.gz")
    mnist_test_dataloader = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                                batch_size=batch_size,
                                                shuffle=False,
                                                collate_fn=ndl.data.collate_mnist,
                                                drop_last=False)

    for i, batch in enumerate(mnist_test_dataloader):
        batch_x, batch_y = batch[0].numpy(), batch[1].numpy()
        truth = mnist_test_dataset[i * batch_size:(i + 1) * batch_size]
        truth_x = truth[0]
        truth_y = truth[1]

        np.testing.assert_allclose(truth_x, batch_x)
        np.testing.assert_allclose(batch_y, truth_y)

    bat9 = ndl.data.DataLoader(dataset=mnist_test_dataset,
                               batch_size=9,
                               shuffle=False,
                               collate_fn=ndl.data.collate_mnist,
                               drop_last=False)
    bat9drop = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                   batch_size=9,
                                   shuffle=False,
                                   collate_fn=ndl.data.collate_mnist,
                                   drop_last=True)
    assert len(bat9) != bat9drop, 'drop_last had no effect.'

    noshuf = bat9 = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                        batch_size=10,
                                        shuffle=False,
                                        collate_fn=ndl.data.collate_mnist,
                                        drop_last=False)
    shuf = bat9 = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                      batch_size=10,
                                      shuffle=True,
                                      collate_fn=ndl.data.collate_mnist,
                                      drop_last=False)
    diff = False
    for i, j in zip(shuf, noshuf):
        if i != j:
            diff = True
            break
    assert diff, 'shuffling had no effect on the dataloader.'


def submit_dataloader():
    batch_size = 1
    mnist_train_dataset = ndl.data.MNISTDataset("data/train-images-idx3-ubyte.gz",
                                                "data/train-labels-idx1-ubyte.gz")
    mnist_train_dataloader = ndl.data.DataLoader(dataset=mnist_train_dataset,
                                                 batch_size=batch_size,
                                                 shuffle=False,
                                                 collate_fn=ndl.data.collate_mnist,
                                                 drop_last=False)
    subl = []
    for i, batch in enumerate(mnist_train_dataloader):
        batch_x, batch_y = batch[0].numpy(), batch[1].numpy()
        subl.append(np.sum(batch_x))
        subl.append(np.sum(batch_y))
        if i > 3:
            break
    mugrade.submit(subl)

    batch_size = 5
    mnist_test_dataset = ndl.data.MNISTDataset("data/t10k-images-idx3-ubyte.gz",
                                               "data/t10k-labels-idx1-ubyte.gz")
    mnist_test_dataloader = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                                batch_size=batch_size,
                                                shuffle=False,
                                                collate_fn=ndl.data.collate_mnist,
                                                drop_last=False)

    subl = []
    for i, batch in enumerate(mnist_test_dataloader):
        batch_x, batch_y = batch[0].numpy(), batch[1].numpy()
        subl.append(np.sum(batch_x))
        subl.append(np.sum(batch_y))

    mugrade.submit(subl[-3:])

    bdrop = ndl.data.DataLoader(dataset=mnist_test_dataset,
                                   batch_size=13,
                                   shuffle=False,
                                   collate_fn=ndl.data.collate_mnist,
                                   drop_last=True)
    mugrade.submit(len(bdrop))

    np.random.seed(0)
    shuf = ndl.data.DataLoader(dataset=mnist_test_dataset,
                               batch_size=10,
                               shuffle=True,
                               collate_fn=ndl.data.collate_mnist,
                               drop_last=False)
    subl = []
    for i, batch in enumerate(shuf):
        batch_x, batch_y = batch[0].numpy(), batch[1].numpy()
        subl.append(np.sum(batch_x))
        subl.append(np.sum(batch_y))
        if i > 3:
            break
    mugrade.submit(subl)



if __name__ == "__main__":
    test_flip_horizontal()
    test_random_crop()
    test_sequential_sampler()
    test_random_sampler()
    test_batch_sampler()
    test_collate_mnist()
    test_mnist_dataset()
    test_dataloader()
